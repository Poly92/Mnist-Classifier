#%%

from fastbook import * 
from fastai.vision.all import * 

from pandas import *

#%% md

# Load dataset

#%% md

##### MNIST

#%%

path = untar_data(URLs.MNIST)
path.ls()

#%%

for i in range(10):
    temp_path = path/'training'/str(i)
    print(len(temp_path.ls()))


#%%

training_data = []
training_labels = []

for i in range(10):
    temp_path = path/'training'/str(i)
    data = [tensor(Image.open(j)) for j in temp_path.ls()]
    training_data.extend(d for d in data)
    training_labels.extend([i]*len(data))
    
training_labels = tensor(training_labels)    

#%%

training_data[20876].shape, training_data[20876], training_labels.shape

#%%

mnist_dset = list(zip(training_data,training_labels))

#%%

mnist_dl = DataLoader(mnist_dl, batch_size=)

#%% md

##### Fashion MNIST

#%%

fashion_mnist_path = Path('/Users/douglous/.fastai/data/fashion_mnist')
fashion_mnist_path.ls()

#%%

df = pd.read_csv(fashion_mnist_path/'fashion-mnist_train.csv')
test_df = pd.read_csv(fashion_mnist_path/'fashion-mnist_test.csv')

#%%

labels = tensor(df.label.values) ; labels, labels.shape
test_labels = tensor(test_df.label.values) ; test_labels, test_labels.shape

#%%

dataset = tensor(df.drop('label',axis=1).values).float(); dataset, dataset.shape
test_dataset = tensor(test_df.drop('label',axis=1).values).float(); test_dataset, test_dataset.shape

#%%

dset = list(zip(dataset, labels))
train_dl = DataLoader(dset, batch_size=200)
test_dset = list(zip(test_dataset, test_labels))
test_dl = DataLoader(test_dset, batch_size=200)

#%% md

## Testing

#%%

data1, label1 = first(train_dl)
w, b = layer1.parameters()
data1.shape, label1.shape, w.shape, b.shape

#%%

q = model(data1)
activated = torch.softmax(q, dim=1)
q.shape, q, activated.shape


#%%

sm_acts = activated[range(200), label1.long()]
sm_acts.shape, sm_acts

#%%

loss = (1-sm_acts).mean(); loss

#%%

F.nll_loss(model(data1), label1)

#%% md

# Initialize Model

#%%

layer1 = nn.Linear(28*28, 300)
layer2 = nn.ReLU()
layer3 = nn.Linear(300, 100)
layer4 = nn.ReLU()
layer5 = nn.Linear(100, 10)
model = nn.Sequential(layer1, layer2, layer3, layer4, layer5)

#%% md

# Trainer

#%%

opt = SGD(model.parameters(),1e-5)

def batch_accuracy(preds, target):
    return torch.where(preds.max(dim=1).indices==target.squeeze(), 1, 0).float().mean()

def calculate_grad(model, x, y):
    preds = model(x)
    loss = nn.CrossEntropyLoss()(preds, yb)
    loss.backward()
    return loss;

#%%

class Optimiser:
    
    def __init__(self, params, lr):
        self.params, self.lr = params, lr
        
    def step(self):
        for p in self.params: p.data -= self.lr * p.grad.data
        
    def zero_grad(self):
        for p in self.params: p.grad.zero_()

#%%

class Trainer:
    
    def __init__(self, dl, opt, cal_grad_fn, model, loss_f): 
        self.dl, self.opt, self.model, self.loss_fn, self.cal_grad_fn = dl, opt, model, loss_f, cal_grad_fn; 
            
    def train(self, epochs):
        for i in range(epochs):
#            print('running for epoch '+ str(i))
            batch_losses=[]
            for xb,yb in self.dl:
                batch_losses.append(self.cal_grad_fn(model,xb,yb))
                opt.step()
                opt.zero_grad()
#            print('Avg training loss: '+ str(tensor(batch_losses).mean()))
            
    def accuracy(self, test_dl):
        accs = [batch_accuracy(self.model(xb.float()), yb) for xb, yb in test_dl]
        return round(tensor(accs).mean().item(), 4)


#%%

trainer = Trainer(train_dl, opt=Optimiser(model.parameters, 1e-2), cal_grad_fn=calculate_grad, model=model, loss_f=F.cross_entropy)

#%%

trainer.train(60)

#%%

j,k= first(train_dl)
sample_input=model(j.float())[4:19]
sample_input, sample_input.max(dim=1).indices, k[4:19]

